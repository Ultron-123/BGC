{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 23:41:07.744415: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-03 23:41:07.783023: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 23:41:07.989558: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 23:41:07.992469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-03 23:41:08.914460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from keras.utils import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib import losses\n",
    "from keras_contrib import metrics\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>protein_id</th>\n",
       "      <th>gene_start</th>\n",
       "      <th>gene_end</th>\n",
       "      <th>gene_strand</th>\n",
       "      <th>pfam_id</th>\n",
       "      <th>in_cluster</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>domain_start</th>\n",
       "      <th>domain_end</th>\n",
       "      <th>bitscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF02353</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF01135</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF01269</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF13489</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF01596</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803357</th>\n",
       "      <td>NEG_FAKE_CLUSTER|U00096.3|AB007043.2</td>\n",
       "      <td>U00096_4155</td>\n",
       "      <td>4452</td>\n",
       "      <td>5399</td>\n",
       "      <td>-1</td>\n",
       "      <td>PF13384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706945.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803358</th>\n",
       "      <td>NEG_FAKE_CLUSTER|U00096.3|AB007043.2</td>\n",
       "      <td>U00096_4155</td>\n",
       "      <td>4452</td>\n",
       "      <td>5399</td>\n",
       "      <td>-1</td>\n",
       "      <td>PF09339</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706946.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803359</th>\n",
       "      <td>NEG_FAKE_CLUSTER|U00096.3|AB007043.2</td>\n",
       "      <td>U00096_4155</td>\n",
       "      <td>4452</td>\n",
       "      <td>5399</td>\n",
       "      <td>-1</td>\n",
       "      <td>PF00532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706947.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>40.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803360</th>\n",
       "      <td>NEG_FAKE_CLUSTER|U00096.3|AB007043.2</td>\n",
       "      <td>U00096_3308</td>\n",
       "      <td>5399</td>\n",
       "      <td>6625</td>\n",
       "      <td>-1</td>\n",
       "      <td>PF01676</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706948.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>169.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803361</th>\n",
       "      <td>NEG_FAKE_CLUSTER|U00096.3|AB007043.2</td>\n",
       "      <td>U00096_3308</td>\n",
       "      <td>5399</td>\n",
       "      <td>6625</td>\n",
       "      <td>-1</td>\n",
       "      <td>PF01663</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>706949.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>23.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>803362 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sequence_id   protein_id  gene_start  \\\n",
       "0                               BGC0000001.1   AEK75490.1           0   \n",
       "1                               BGC0000001.1   AEK75490.1           0   \n",
       "2                               BGC0000001.1   AEK75490.1           0   \n",
       "3                               BGC0000001.1   AEK75490.1           0   \n",
       "4                               BGC0000001.1   AEK75490.1           0   \n",
       "...                                      ...          ...         ...   \n",
       "803357  NEG_FAKE_CLUSTER|U00096.3|AB007043.2  U00096_4155        4452   \n",
       "803358  NEG_FAKE_CLUSTER|U00096.3|AB007043.2  U00096_4155        4452   \n",
       "803359  NEG_FAKE_CLUSTER|U00096.3|AB007043.2  U00096_4155        4452   \n",
       "803360  NEG_FAKE_CLUSTER|U00096.3|AB007043.2  U00096_3308        5399   \n",
       "803361  NEG_FAKE_CLUSTER|U00096.3|AB007043.2  U00096_3308        5399   \n",
       "\n",
       "        gene_end  gene_strand  pfam_id  in_cluster  label  Unnamed: 0  \\\n",
       "0           1083            1  PF02353           1      1         NaN   \n",
       "1           1083            1  PF01135           1      1         NaN   \n",
       "2           1083            1  PF01269           1      1         NaN   \n",
       "3           1083            1  PF13489           1      1         NaN   \n",
       "4           1083            1  PF01596           1      1         NaN   \n",
       "...          ...          ...      ...         ...    ...         ...   \n",
       "803357      5399           -1  PF13384           0      0    706945.0   \n",
       "803358      5399           -1  PF09339           0      0    706946.0   \n",
       "803359      5399           -1  PF00532           0      0    706947.0   \n",
       "803360      6625           -1  PF01676           0      0    706948.0   \n",
       "803361      6625           -1  PF01663           0      0    706949.0   \n",
       "\n",
       "        domain_start  domain_end  bitscore  \n",
       "0                NaN         NaN       NaN  \n",
       "1                NaN         NaN       NaN  \n",
       "2                NaN         NaN       NaN  \n",
       "3                NaN         NaN       NaN  \n",
       "4                NaN         NaN       NaN  \n",
       "...              ...         ...       ...  \n",
       "803357          16.0        49.0      18.5  \n",
       "803358          17.0        40.0      16.5  \n",
       "803359          22.0       274.0      40.2  \n",
       "803360           1.0       243.0     169.8  \n",
       "803361         142.0       237.0      23.3  \n",
       "\n",
       "[803362 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data = pd.read_csv('MIBiG.pfam.tsv', sep='\\t')\n",
    "negative_data = pd.read_csv('GeneSwap_Negatives.pfam.tsv', sep='\\t')\n",
    "negative_data.rename(columns={\"contig_id\": \"sequence_id\"}, inplace=True)\n",
    "\n",
    "# Combine datasets\n",
    "positive_data['label'] = 1\n",
    "negative_data['label'] = 0\n",
    "combined_data = pd.concat([positive_data, negative_data], ignore_index=True)\n",
    "combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_ids = combined_data['pfam_id'].unique()\n",
    "pfam_to_idx = {pfam: idx for idx, pfam in enumerate(pfam_ids)}\n",
    "combined_data['pfam_idx'] = combined_data['pfam_id'].map(pfam_to_idx)\n",
    "\n",
    "# Prepare sequences\n",
    "sequences = combined_data.groupby('sequence_id')['pfam_idx'].apply(list).values\n",
    "labels = combined_data.groupby('sequence_id')['label'].first().values\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build BiLSTM-CRF model\n",
    "input_dim = len(pfam_ids)\n",
    "output_dim = 50  # Embedding dimension\n",
    "input_length = max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 373)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 373, 50)           481650    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 373, 100)         40400     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 373, 2)           202       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " crf (CRF)                   (None, 373, 2)            14        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 522,266\n",
      "Trainable params: 522,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "WORD_COUNT = len(pfam_ids)  # Vocabulary size\n",
    "DENSE_EMBEDDING = 50  # Embedding dimension\n",
    "LSTM_UNITS = 50  # Number of LSTM units\n",
    "LSTM_DROPOUT = 0.1  # LSTM dropout rate\n",
    "DENSE_UNITS = 2  # Number of output classes (0 and 1)\n",
    "TAG_COUNT = 2  # Number of output classes (0 and 1)\n",
    "BATCH_SIZE = 32  # Batch size\n",
    "MAX_EPOCHS = 10  # Number of epochs\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_len,))\n",
    "\n",
    "# Embedding layer\n",
    "model = Embedding(input_dim=WORD_COUNT, output_dim=DENSE_EMBEDDING, input_length=max_len)(input_layer)\n",
    "\n",
    "# Bidirectional LSTM layer\n",
    "model = Bidirectional(LSTM(units=LSTM_UNITS, recurrent_dropout=LSTM_DROPOUT, return_sequences=True))(model)\n",
    "\n",
    "# TimeDistributed Dense layer\n",
    "model = TimeDistributed(Dense(DENSE_UNITS, activation=\"relu\"))(model)\n",
    "\n",
    "# CRF layer\n",
    "crf_layer = CRF(units=TAG_COUNT)\n",
    "output_layer = crf_layer(model)\n",
    "\n",
    "# Define the model\n",
    "ner_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "loss = losses.crf_loss\n",
    "acc_metric = metrics.crf_accuracy\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "ner_model.compile(optimizer=opt, loss=loss, metrics=[acc_metric])\n",
    "\n",
    "# Print model summary\n",
    "ner_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/home/ultron/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ultron/.local/lib/python3.10/site-packages/keras_contrib/losses/crf_losses.py\", line 54, in crf_loss  *\n        crf, idx = y_pred._keras_history[:2]\n\n    AttributeError: 'Tensor' object has no attribute '_keras_history'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m     ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mner_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filerjl2w6jk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_70aawq9.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__crf_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     23\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 25\u001b[0m (crf, idx) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_history\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_state_1\u001b[39m():\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/home/ultron/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ultron/.local/lib/python3.10/site-packages/keras_contrib/losses/crf_losses.py\", line 54, in crf_loss  *\n        crf, idx = y_pred._keras_history[:2]\n\n    AttributeError: 'Tensor' object has no attribute '_keras_history'\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = ner_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CRF' object has no attribute 'get_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39moutput_layer)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# ✅ Fix Loss Function\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcrf_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m()  \u001b[38;5;66;03m# ✅ Correct\u001b[39;00m\n\u001b[1;32m     71\u001b[0m acc_metric \u001b[38;5;241m=\u001b[39m crf_layer\u001b[38;5;241m.\u001b[39mget_accuracy()  \u001b[38;5;66;03m# ✅ Correct\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CRF' object has no attribute 'get_loss'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "# Load datasets\n",
    "positive_data = pd.read_csv('MIBiG.pfam.tsv', sep='\\t')\n",
    "negative_data = pd.read_csv('GeneSwap_Negatives.pfam.tsv', sep='\\t')\n",
    "negative_data.rename(columns={\"contig_id\": \"sequence_id\"}, inplace=True)\n",
    "\n",
    "# Combine datasets\n",
    "positive_data['label'] = 1\n",
    "negative_data['label'] = 0\n",
    "combined_data = pd.concat([positive_data, negative_data], ignore_index=True)\n",
    "\n",
    "# Encode Pfam IDs\n",
    "pfam_ids = combined_data['pfam_id'].unique()\n",
    "pfam_to_idx = {pfam: idx for idx, pfam in enumerate(pfam_ids)}\n",
    "combined_data['pfam_idx'] = combined_data['pfam_id'].map(pfam_to_idx)\n",
    "\n",
    "# Prepare sequences\n",
    "sequences = combined_data.groupby('sequence_id')['pfam_idx'].apply(list).values\n",
    "labels = combined_data.groupby('sequence_id')['label'].first().values\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=1234)\n",
    "\n",
    "# Define model parameters\n",
    "WORD_COUNT = len(pfam_ids)  \n",
    "DENSE_EMBEDDING = 50  \n",
    "LSTM_UNITS = 50  \n",
    "LSTM_DROPOUT = 0.1  \n",
    "DENSE_UNITS = 2  \n",
    "TAG_COUNT = 2  \n",
    "BATCH_SIZE = 32  \n",
    "MAX_EPOCHS = 10  \n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(max_len,))\n",
    "\n",
    "# Embedding layer\n",
    "model = Embedding(input_dim=WORD_COUNT, output_dim=DENSE_EMBEDDING, input_length=max_len)(input_layer)\n",
    "\n",
    "# Bidirectional LSTM layer\n",
    "model = Bidirectional(LSTM(units=LSTM_UNITS, recurrent_dropout=LSTM_DROPOUT, return_sequences=True))(model)\n",
    "\n",
    "# TimeDistributed Dense layer\n",
    "model = TimeDistributed(Dense(DENSE_UNITS, activation=\"relu\"))(model)\n",
    "\n",
    "# CRF layer\n",
    "crf_layer = CRF(TAG_COUNT)\n",
    "output_layer = crf_layer(model)\n",
    "\n",
    "# Define the model\n",
    "ner_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# ✅ Fix Loss Function\n",
    "loss = crf_layer.get_loss()  # ✅ Correct\n",
    "acc_metric = crf_layer.get_accuracy()  # ✅ Correct\n",
    "\n",
    "# Compile the model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "ner_model.compile(optimizer=opt, loss=loss, metrics=[acc_metric])\n",
    "\n",
    "# Print model summary\n",
    "ner_model.summary()\n",
    "\n",
    "# Add callbacks for early stopping and model checkpointing\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = ner_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_sequences = test_data['pfam_id'].map(pfam_to_idx).values\n",
    "test_X = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict\n",
    "predictions = ner_model.predict(test_X)\n",
    "predicted_labels = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Output predictions\n",
    "test_data['predicted_label'] = predicted_labels.flatten()\n",
    "test_data.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 00:22:09.725213: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-03-04 00:22:09.725998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-03-04 00:22:09.727166: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-03-04 00:22:09.815732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-03-04 00:22:09.843412: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-03-04 00:22:09.844203: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-03-04 00:22:09.845410: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m crf_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# ✅ Summary\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43mcrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:3229\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m \n\u001b[1;32m   3200\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3233\u001b[0m     )\n\u001b[1;32m   3234\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3236\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3241\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3242\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.text.crf import crf_log_likelihood\n",
    "\n",
    "\n",
    "# Helper function to unpack data\n",
    "def unpack_data(data):\n",
    "    if isinstance(data, tuple):\n",
    "        return data if len(data) == 3 else (data[0], data[1], None)\n",
    "    return data, None, None\n",
    "\n",
    "\n",
    "# ✅ Custom CRF Model Wrapper\n",
    "class ModelWithCRFLoss(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.base_model(inputs)\n",
    "\n",
    "    def compute_loss(self, x, y, sample_weight=None, training=False):\n",
    "        y_pred = self(x, training=training)\n",
    "        potentials, sequence_length, chain_kernel = y_pred\n",
    "\n",
    "        # Compute CRF loss\n",
    "        crf_loss = -crf_log_likelihood(potentials, y, sequence_length, chain_kernel)[0]\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            crf_loss *= sample_weight\n",
    "\n",
    "        return tf.reduce_mean(crf_loss) + sum(self.losses)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = unpack_data(data)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = self.compute_loss(x, y, sample_weight, training=True)\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": total_loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = unpack_data(data)\n",
    "        total_loss = self.compute_loss(x, y, sample_weight)\n",
    "        return {\"val_loss\": total_loss}\n",
    "\n",
    "\n",
    "# ✅ Define Model\n",
    "WORD_COUNT = 1000  # Example vocabulary size\n",
    "DENSE_EMBEDDING = 50\n",
    "LSTM_UNITS = 50\n",
    "MAX_LEN = 100\n",
    "TAG_COUNT = 2  # Number of output labels\n",
    "\n",
    "input_layer = Input(shape=(MAX_LEN,))\n",
    "embedding_layer = Embedding(input_dim=WORD_COUNT, output_dim=DENSE_EMBEDDING, input_length=MAX_LEN)(input_layer)\n",
    "bi_lstm_layer = Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True))(embedding_layer)\n",
    "dense_layer = Dense(TAG_COUNT)(bi_lstm_layer)\n",
    "\n",
    "# ✅ Apply CRF correctly\n",
    "crf_layer = CRF(TAG_COUNT)\n",
    "output_layer = crf_layer(dense_layer)\n",
    "\n",
    "# ✅ Wrap the model with CRF Loss Handling\n",
    "base_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "crf_model = ModelWithCRFLoss(base_model)\n",
    "\n",
    "# ✅ Compile with Optimizer\n",
    "crf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# ✅ Summary\n",
    "crf_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
