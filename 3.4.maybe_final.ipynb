{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load positive and negative datasets\n",
    "positive_data = pd.read_csv(\"MIBiG.pfam.tsv\", sep=\"\\t\")\n",
    "negative_data = pd.read_csv(\"GeneSwap_Negatives.pfam.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Add labels: 1 for positive (BGC), 0 for negative (non-BGC)\n",
    "positive_data[\"label\"] = 1\n",
    "negative_data[\"label\"] = 0\n",
    "\n",
    "# Combine datasets\n",
    "combined_data = pd.concat([positive_data, negative_data], ignore_index=True)\n",
    "\n",
    "# Create vocabulary of Pfam IDs\n",
    "vocab = set(combined_data[\"pfam_id\"].unique())\n",
    "vocab_size = 9633  # As provided\n",
    "pfam_to_idx = {pfam: idx + 1 for idx, pfam in enumerate(vocab)}  # +1 to reserve 0 for padding\n",
    "idx_to_pfam = {idx: pfam for pfam, idx in pfam_to_idx.items()}\n",
    "\n",
    "# Group by sequence_id or contig_id to create sequences\n",
    "def create_sequences(data, group_col, pfam_col, label_col):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for group, group_data in data.groupby(group_col):\n",
    "        sequences.append([pfam_to_idx[pfam] for pfam in group_data[pfam_col]])\n",
    "        labels.append(group_data[label_col].iloc[0])  # All rows in a group have the same label\n",
    "    return sequences, labels\n",
    "\n",
    "# Create sequences for positive and negative data\n",
    "positive_sequences, positive_labels = create_sequences(positive_data, \"sequence_id\", \"pfam_id\", \"label\")\n",
    "negative_sequences, negative_labels = create_sequences(negative_data, \"contig_id\", \"pfam_id\", \"label\")\n",
    "\n",
    "# Combine sequences and labels\n",
    "all_sequences = positive_sequences + negative_sequences\n",
    "all_labels = positive_labels + negative_labels\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_seq_length = 373  # As provided\n",
    "padded_sequences = [seq + [0] * (max_seq_length - len(seq)) for seq in all_sequences]  # 0 is padding index\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "y = torch.tensor(all_labels, dtype=torch.long)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "class PfamDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = PfamDataset(X_train, y_train)\n",
    "val_dataset = PfamDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(BiLSTM_Classifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)  # +1 for padding\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)  # Fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        lstm_out, _ = self.bilstm(embeds)  # Shape: (batch_size, sequence_length, hidden_dim)\n",
    "        \n",
    "        # Global average pooling over the sequence length\n",
    "        pooled = lstm_out.mean(dim=1)  # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Final classification layer\n",
    "        logits = self.fc(pooled)  # Shape: (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Define model hyperparameters\n",
    "vocab_size = 9633  # As provided\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_classes = 2  # Binary classification: BGC (1) or non-BGC (0)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = BiLSTM_Classifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.24915999892127788\n",
      "Validation Loss: 0.12423254240696367, Accuracy: 0.945934791580685\n",
      "Epoch 2/10, Loss: 0.09412145120992323\n",
      "Validation Loss: 0.08217714595191769, Accuracy: 0.976062732150227\n",
      "Epoch 3/10, Loss: 0.05743977088840808\n",
      "Validation Loss: 0.0791475266696101, Accuracy: 0.9715229054890632\n",
      "Epoch 4/10, Loss: 0.037460970719999605\n",
      "Validation Loss: 0.06545809579792579, Accuracy: 0.9773008666941808\n",
      "Epoch 5/10, Loss: 0.030582942562285356\n",
      "Validation Loss: 0.06785109426709823, Accuracy: 0.977713578208832\n",
      "Epoch 6/10, Loss: 0.02289315675412287\n",
      "Validation Loss: 0.07286800754158512, Accuracy: 0.9801898472967395\n",
      "Epoch 7/10, Loss: 0.0165732242337969\n",
      "Validation Loss: 0.0643896695715069, Accuracy: 0.9830788278992983\n",
      "Epoch 8/10, Loss: 0.014251269141787266\n",
      "Validation Loss: 0.0704914813127528, Accuracy: 0.9797771357820884\n",
      "Epoch 9/10, Loss: 0.023953468759692884\n",
      "Validation Loss: 0.08446902422675569, Accuracy: 0.9739991745769707\n",
      "Epoch 10/10, Loss: 0.01843862373650031\n",
      "Validation Loss: 0.059702422049380595, Accuracy: 0.9806025588113908\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)  # Shape: (batch_size, num_classes)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)  # labels shape: (batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            outputs = model(sequences)  # Shape: (batch_size, num_classes)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader)}, Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted BGC Clusters: ['PF14871']\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_sequences = [[pfam_to_idx[pfam] for pfam in test_data[\"pfam_id\"]]]\n",
    "padded_test_sequences = [seq + [0] * (max_seq_length - len(seq)) for seq in test_sequences]\n",
    "X_test = torch.tensor(padded_test_sequences, dtype=torch.long)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)  # Shape: (batch_size, num_classes)\n",
    "    _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = predicted.tolist()\n",
    "predicted_bgc = [idx_to_pfam[idx] for idx in predicted_labels if idx != 0]  # Exclude padding\n",
    "\n",
    "# Output predicted BGC clusters\n",
    "print(\"Predicted BGC Clusters:\", predicted_bgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)  # +1 for padding\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)  # Fully connected layer for tag scores\n",
    "        self.crf = CRF(num_classes, batch_first=True)  # CRF layer\n",
    "\n",
    "    def forward(self, x, labels=None):  # Add `labels` as an optional argument\n",
    "        embeds = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        lstm_out, _ = self.bilstm(embeds)  # Shape: (batch_size, sequence_length, hidden_dim)\n",
    "        tag_scores = self.fc(lstm_out)  # Shape: (batch_size, sequence_length, num_classes)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Training: Compute CRF loss\n",
    "            loss = -self.crf(tag_scores, labels)  # Negative log likelihood\n",
    "            return loss\n",
    "        else:\n",
    "            # Inference: Decode the best sequence of tags\n",
    "            return self.crf.decode(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "vocab_size = 9633  # As provided\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_classes = 2  # Binary classification: BGC (1) or non-BGC (0)\n",
    "\n",
    "# Initialize the BiLSTM_CRF model\n",
    "model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the first two dimensions of emissions and tags must match, got (32, 373) and (32,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute CRF loss\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass labels to compute CRF loss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mBiLSTM_CRF.forward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m tag_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out)  \u001b[38;5;66;03m# Shape: (batch_size, sequence_length, num_classes)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Training: Compute CRF loss\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Negative log likelihood\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Inference: Decode the best sequence of tags\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchcrf/__init__.py:90\u001b[0m, in \u001b[0;36mCRF.forward\u001b[0;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     65\u001b[0m         emissions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m         reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m        reduction is ``none``, ``()`` otherwise.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_mean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid reduction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchcrf/__init__.py:155\u001b[0m, in \u001b[0;36mCRF._validate\u001b[0;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m tags\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe first two dimensions of emissions and tags must match, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(tags\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emissions\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m mask\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[0;31mValueError\u001b[0m: the first two dimensions of emissions and tags must match, got (32, 373) and (32,)"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute CRF loss\n",
    "        loss = model(sequences, labels)  # Pass labels to compute CRF loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            # Forward pass: Compute CRF loss\n",
    "            loss = model(sequences, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Decode the best sequence of tags\n",
    "            predicted_tags = model(sequences)  # Shape: (batch_size, sequence_length)\n",
    "            \n",
    "            # Flatten labels and predictions for accuracy calculation\n",
    "            true_tags = labels.view(-1).tolist()\n",
    "            pred_tags = [tag for seq in predicted_tags for tag in seq]\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            correct += sum(t1 == t2 for t1, t2 in zip(true_tags, pred_tags))\n",
    "            total += len(true_tags)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader)}, Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
