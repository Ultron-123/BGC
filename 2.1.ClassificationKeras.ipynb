{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 14:32:08.404017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 14:32:08.404654: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-14 14:32:08.406979: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-14 14:32:08.414554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739523728.426809   58393 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739523728.430343   58393 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 14:32:08.442921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, ShuffleSplit, KFold\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.layers import Dense, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 128\n",
    "BATCH_SIZE = 16\n",
    "PFAM2VEC_DIMENSIONS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alkaloid</th>\n",
       "      <th>NRP</th>\n",
       "      <th>Other</th>\n",
       "      <th>Polyketide</th>\n",
       "      <th>RiPP</th>\n",
       "      <th>Saccharide</th>\n",
       "      <th>Terpene</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contig_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BGC0000001.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0000002.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0000003.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0000004.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0000005.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0001826.1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0001827.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0001828.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0001829.1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGC0001830.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2018 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Alkaloid  NRP  Other  Polyketide  RiPP  Saccharide  Terpene\n",
       "contig_id                                                                \n",
       "BGC0000001.1         0    0      0           1     0           0        0\n",
       "BGC0000002.1         0    0      0           1     0           0        0\n",
       "BGC0000003.1         0    0      0           1     0           0        0\n",
       "BGC0000004.1         0    0      0           1     0           0        0\n",
       "BGC0000005.1         0    0      0           1     0           0        0\n",
       "...                ...  ...    ...         ...   ...         ...      ...\n",
       "BGC0001826.1         0    1      0           0     0           0        0\n",
       "BGC0001827.1         0    0      0           1     0           0        0\n",
       "BGC0001828.1         0    0      0           1     0           0        0\n",
       "BGC0001829.1         0    1      0           1     0           0        0\n",
       "BGC0001830.1         0    0      0           1     0           0        0\n",
       "\n",
       "[2018 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"MIBiG.classes.csv\").set_index('contig_id')\n",
    "NUM_LABELS = labels.shape[1]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alkaloid       54\n",
       "NRP           603\n",
       "Other         247\n",
       "Polyketide    849\n",
       "RiPP          261\n",
       "Saccharide    187\n",
       "Terpene       167\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts per class:\n",
      "Alkaloid       54\n",
      "NRP           603\n",
      "Other         247\n",
      "Polyketide    849\n",
      "RiPP          261\n",
      "Saccharide    187\n",
      "Terpene       167\n",
      "dtype: int64\n",
      "\n",
      "Total counts of overlapping BGCs per class:\n",
      "Alkaloid       12\n",
      "NRP           265\n",
      "Other          11\n",
      "Polyketide    308\n",
      "RiPP            1\n",
      "Saccharide     54\n",
      "Terpene        36\n",
      "dtype: int64\n",
      "\n",
      "Total number of BGCs with overlaps: 337\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of BGCs per class\n",
    "label_counts = labels.sum()\n",
    "\n",
    "# Count the number of BGCs that belong to multiple classes\n",
    "overlap_counts = labels[labels.sum(axis=1) > 1].sum()\n",
    "\n",
    "# Count the number of unique overlaps\n",
    "num_overlapping_bgcs = (labels.sum(axis=1) > 1).sum()\n",
    "\n",
    "print(\"Total counts per class:\")\n",
    "print(label_counts)\n",
    "\n",
    "print(\"\\nTotal counts of overlapping BGCs per class:\")\n",
    "print(overlap_counts)\n",
    "\n",
    "print(f\"\\nTotal number of BGCs with overlaps: {num_overlapping_bgcs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts per class:\n",
      "Alkaloid       54\n",
      "NRP           603\n",
      "Other         247\n",
      "Polyketide    849\n",
      "RiPP          261\n",
      "Saccharide    187\n",
      "Terpene       167\n",
      "dtype: int64\n",
      "\n",
      "Total number of BGCs with overlaps: 337\n",
      "\n",
      "Overlap matrix (how often classes co-occur):\n",
      "            Alkaloid  NRP  Other  Polyketide  RiPP  Saccharide  Terpene\n",
      "Alkaloid          12    6      0           3     0           0        6\n",
      "NRP                6  265      5         251     1          10        2\n",
      "Other              0    5     11           4     0           6        0\n",
      "Polyketide         3  251      4         308     0          40       22\n",
      "RiPP               0    1      0           0     1           0        0\n",
      "Saccharide         0   10      6          40     0          54        7\n",
      "Terpene            6    2      0          22     0           7       36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labels\n",
    "labels = pd.read_csv(\"MIBiG.classes.csv\").set_index('contig_id')\n",
    "\n",
    "# Count occurrences of each class\n",
    "label_counts = labels.sum()\n",
    "\n",
    "# Find BGCs that belong to multiple classes\n",
    "overlapping_bgcs = labels[labels.sum(axis=1) > 1]\n",
    "\n",
    "# Count how often each class overlaps with others\n",
    "overlap_matrix = overlapping_bgcs.T.dot(overlapping_bgcs)\n",
    "\n",
    "print(\"Total counts per class:\")\n",
    "print(label_counts)\n",
    "\n",
    "print(\"\\nTotal number of BGCs with overlaps:\", len(overlapping_bgcs))\n",
    "\n",
    "print(\"\\nOverlap matrix (how often classes co-occur):\")\n",
    "print(overlap_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BGCs with more than 3 labels: 13\n",
      "              Alkaloid  NRP  Other  Polyketide  RiPP  Saccharide  Terpene\n",
      "contig_id                                                                \n",
      "BGC0000119.1         0    0      1           1     0           1        0\n",
      "BGC0000269.1         0    0      1           1     0           1        0\n",
      "BGC0000297.1         0    1      0           1     0           1        0\n",
      "BGC0000963.1         0    1      0           1     0           1        0\n",
      "BGC0000978.1         1    1      0           1     0           0        0\n",
      "BGC0001003.1         0    1      0           1     0           1        0\n",
      "BGC0001046.1         0    1      0           1     0           1        0\n",
      "BGC0001048.1         0    1      0           1     0           1        0\n",
      "BGC0001056.1         0    1      1           1     0           0        0\n",
      "BGC0001058.1         0    1      0           1     0           1        0\n",
      "BGC0001072.1         0    0      1           1     0           1        0\n",
      "BGC0001084.1         1    1      0           0     0           0        1\n",
      "BGC0001449.1         1    1      0           1     0           0        0\n"
     ]
    }
   ],
   "source": [
    "# Find BGCs where the sum of ones (labels) per row is greater than 3\n",
    "bgcs_with_more_than_3_labels = labels[labels.sum(axis=1) >= 3]\n",
    "\n",
    "# Count how many such rows exist\n",
    "num_bgcs_with_more_than_3_labels = bgcs_with_more_than_3_labels.shape[0]\n",
    "\n",
    "# Display the rows\n",
    "print(f\"Number of BGCs with more than 3 labels: {num_bgcs_with_more_than_3_labels}\")\n",
    "print(bgcs_with_more_than_3_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PF00005</th>\n",
       "      <td>0.233514</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.033491</td>\n",
       "      <td>0.240617</td>\n",
       "      <td>-0.052133</td>\n",
       "      <td>-0.383191</td>\n",
       "      <td>0.097324</td>\n",
       "      <td>0.307650</td>\n",
       "      <td>-0.516893</td>\n",
       "      <td>-0.119899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130947</td>\n",
       "      <td>-0.349238</td>\n",
       "      <td>-0.052996</td>\n",
       "      <td>0.266614</td>\n",
       "      <td>0.556849</td>\n",
       "      <td>-0.025646</td>\n",
       "      <td>0.321105</td>\n",
       "      <td>-0.287103</td>\n",
       "      <td>0.029798</td>\n",
       "      <td>0.149110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF07690</th>\n",
       "      <td>0.084144</td>\n",
       "      <td>0.282901</td>\n",
       "      <td>-0.045280</td>\n",
       "      <td>0.076797</td>\n",
       "      <td>-0.022403</td>\n",
       "      <td>-0.326592</td>\n",
       "      <td>0.109536</td>\n",
       "      <td>0.147560</td>\n",
       "      <td>0.070727</td>\n",
       "      <td>-0.395535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271868</td>\n",
       "      <td>-0.034904</td>\n",
       "      <td>-0.008765</td>\n",
       "      <td>-0.342772</td>\n",
       "      <td>-0.019638</td>\n",
       "      <td>-0.114983</td>\n",
       "      <td>-0.203931</td>\n",
       "      <td>-0.463244</td>\n",
       "      <td>0.079770</td>\n",
       "      <td>0.186862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF13304</th>\n",
       "      <td>0.111681</td>\n",
       "      <td>0.023277</td>\n",
       "      <td>0.181347</td>\n",
       "      <td>0.151265</td>\n",
       "      <td>-0.179347</td>\n",
       "      <td>-0.140607</td>\n",
       "      <td>0.081777</td>\n",
       "      <td>0.406897</td>\n",
       "      <td>-0.509895</td>\n",
       "      <td>-0.132071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100485</td>\n",
       "      <td>-0.220790</td>\n",
       "      <td>-0.087275</td>\n",
       "      <td>0.204365</td>\n",
       "      <td>0.523665</td>\n",
       "      <td>-0.074779</td>\n",
       "      <td>0.398022</td>\n",
       "      <td>-0.301283</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.046756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00072</th>\n",
       "      <td>-0.303923</td>\n",
       "      <td>-0.144928</td>\n",
       "      <td>0.174792</td>\n",
       "      <td>0.111320</td>\n",
       "      <td>-0.050851</td>\n",
       "      <td>-0.241886</td>\n",
       "      <td>0.114559</td>\n",
       "      <td>0.242707</td>\n",
       "      <td>0.023629</td>\n",
       "      <td>-0.339719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259481</td>\n",
       "      <td>-0.173986</td>\n",
       "      <td>-0.335270</td>\n",
       "      <td>-0.064067</td>\n",
       "      <td>0.594384</td>\n",
       "      <td>-0.297773</td>\n",
       "      <td>-0.032209</td>\n",
       "      <td>-0.532959</td>\n",
       "      <td>0.102281</td>\n",
       "      <td>0.016370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00528</th>\n",
       "      <td>-0.073311</td>\n",
       "      <td>0.541115</td>\n",
       "      <td>-0.048413</td>\n",
       "      <td>-0.371761</td>\n",
       "      <td>0.181419</td>\n",
       "      <td>0.124935</td>\n",
       "      <td>0.017770</td>\n",
       "      <td>0.555671</td>\n",
       "      <td>-0.512254</td>\n",
       "      <td>-0.206273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077985</td>\n",
       "      <td>-0.153859</td>\n",
       "      <td>0.116167</td>\n",
       "      <td>0.072122</td>\n",
       "      <td>0.970655</td>\n",
       "      <td>0.079777</td>\n",
       "      <td>0.441720</td>\n",
       "      <td>-0.321656</td>\n",
       "      <td>0.043775</td>\n",
       "      <td>0.187739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF00518</th>\n",
       "      <td>-0.070158</td>\n",
       "      <td>-0.065413</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.022858</td>\n",
       "      <td>0.069239</td>\n",
       "      <td>-0.179477</td>\n",
       "      <td>0.119020</td>\n",
       "      <td>0.293031</td>\n",
       "      <td>-0.051709</td>\n",
       "      <td>-0.138381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104844</td>\n",
       "      <td>-0.152967</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>0.091196</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.022964</td>\n",
       "      <td>0.043654</td>\n",
       "      <td>-0.201326</td>\n",
       "      <td>0.148341</td>\n",
       "      <td>0.097215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF07033</th>\n",
       "      <td>-0.010519</td>\n",
       "      <td>0.020789</td>\n",
       "      <td>-0.006451</td>\n",
       "      <td>-0.060890</td>\n",
       "      <td>-0.009975</td>\n",
       "      <td>-0.159964</td>\n",
       "      <td>0.054209</td>\n",
       "      <td>0.259067</td>\n",
       "      <td>-0.032121</td>\n",
       "      <td>-0.150636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011022</td>\n",
       "      <td>0.020476</td>\n",
       "      <td>0.119876</td>\n",
       "      <td>0.037193</td>\n",
       "      <td>0.120064</td>\n",
       "      <td>-0.122264</td>\n",
       "      <td>0.046126</td>\n",
       "      <td>-0.287325</td>\n",
       "      <td>0.053798</td>\n",
       "      <td>-0.106282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF15873</th>\n",
       "      <td>-0.110694</td>\n",
       "      <td>-0.012107</td>\n",
       "      <td>0.059395</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>0.106874</td>\n",
       "      <td>-0.165131</td>\n",
       "      <td>-0.093995</td>\n",
       "      <td>0.093994</td>\n",
       "      <td>0.076426</td>\n",
       "      <td>-0.070573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111926</td>\n",
       "      <td>-0.116411</td>\n",
       "      <td>0.112473</td>\n",
       "      <td>0.085117</td>\n",
       "      <td>0.157332</td>\n",
       "      <td>0.065394</td>\n",
       "      <td>0.070263</td>\n",
       "      <td>-0.179310</td>\n",
       "      <td>0.149701</td>\n",
       "      <td>-0.101817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF15049</th>\n",
       "      <td>-0.126276</td>\n",
       "      <td>0.086068</td>\n",
       "      <td>0.140609</td>\n",
       "      <td>-0.026558</td>\n",
       "      <td>0.106151</td>\n",
       "      <td>-0.213212</td>\n",
       "      <td>-0.017060</td>\n",
       "      <td>0.259988</td>\n",
       "      <td>0.099858</td>\n",
       "      <td>-0.165359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014387</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>0.023920</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.090026</td>\n",
       "      <td>0.048761</td>\n",
       "      <td>0.020485</td>\n",
       "      <td>-0.080630</td>\n",
       "      <td>0.066559</td>\n",
       "      <td>-0.094392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PF11600</th>\n",
       "      <td>-0.074564</td>\n",
       "      <td>0.014778</td>\n",
       "      <td>-0.031347</td>\n",
       "      <td>-0.028733</td>\n",
       "      <td>-0.029452</td>\n",
       "      <td>-0.124732</td>\n",
       "      <td>0.179168</td>\n",
       "      <td>0.282231</td>\n",
       "      <td>0.095507</td>\n",
       "      <td>0.014530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014614</td>\n",
       "      <td>-0.124625</td>\n",
       "      <td>0.234928</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>0.198487</td>\n",
       "      <td>-0.043470</td>\n",
       "      <td>0.018098</td>\n",
       "      <td>-0.116805</td>\n",
       "      <td>-0.115569</td>\n",
       "      <td>0.051166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12398 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "PF00005  0.233514  0.052725  0.033491  0.240617 -0.052133 -0.383191  0.097324   \n",
       "PF07690  0.084144  0.282901 -0.045280  0.076797 -0.022403 -0.326592  0.109536   \n",
       "PF13304  0.111681  0.023277  0.181347  0.151265 -0.179347 -0.140607  0.081777   \n",
       "PF00072 -0.303923 -0.144928  0.174792  0.111320 -0.050851 -0.241886  0.114559   \n",
       "PF00528 -0.073311  0.541115 -0.048413 -0.371761  0.181419  0.124935  0.017770   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "PF00518 -0.070158 -0.065413  0.019068  0.022858  0.069239 -0.179477  0.119020   \n",
       "PF07033 -0.010519  0.020789 -0.006451 -0.060890 -0.009975 -0.159964  0.054209   \n",
       "PF15873 -0.110694 -0.012107  0.059395 -0.005941  0.106874 -0.165131 -0.093995   \n",
       "PF15049 -0.126276  0.086068  0.140609 -0.026558  0.106151 -0.213212 -0.017060   \n",
       "PF11600 -0.074564  0.014778 -0.031347 -0.028733 -0.029452 -0.124732  0.179168   \n",
       "\n",
       "               7         8         9   ...        90        91        92  \\\n",
       "PF00005  0.307650 -0.516893 -0.119899  ... -0.130947 -0.349238 -0.052996   \n",
       "PF07690  0.147560  0.070727 -0.395535  ...  0.271868 -0.034904 -0.008765   \n",
       "PF13304  0.406897 -0.509895 -0.132071  ... -0.100485 -0.220790 -0.087275   \n",
       "PF00072  0.242707  0.023629 -0.339719  ...  0.259481 -0.173986 -0.335270   \n",
       "PF00528  0.555671 -0.512254 -0.206273  ...  0.077985 -0.153859  0.116167   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "PF00518  0.293031 -0.051709 -0.138381  ...  0.104844 -0.152967  0.031874   \n",
       "PF07033  0.259067 -0.032121 -0.150636  ... -0.011022  0.020476  0.119876   \n",
       "PF15873  0.093994  0.076426 -0.070573  ...  0.111926 -0.116411  0.112473   \n",
       "PF15049  0.259988  0.099858 -0.165359  ... -0.014387 -0.078912  0.023920   \n",
       "PF11600  0.282231  0.095507  0.014530  ... -0.014614 -0.124625  0.234928   \n",
       "\n",
       "               93        94        95        96        97        98        99  \n",
       "PF00005  0.266614  0.556849 -0.025646  0.321105 -0.287103  0.029798  0.149110  \n",
       "PF07690 -0.342772 -0.019638 -0.114983 -0.203931 -0.463244  0.079770  0.186862  \n",
       "PF13304  0.204365  0.523665 -0.074779  0.398022 -0.301283  0.035700  0.046756  \n",
       "PF00072 -0.064067  0.594384 -0.297773 -0.032209 -0.532959  0.102281  0.016370  \n",
       "PF00528  0.072122  0.970655  0.079777  0.441720 -0.321656  0.043775  0.187739  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "PF00518  0.091196 -0.031648 -0.022964  0.043654 -0.201326  0.148341  0.097215  \n",
       "PF07033  0.037193  0.120064 -0.122264  0.046126 -0.287325  0.053798 -0.106282  \n",
       "PF15873  0.085117  0.157332  0.065394  0.070263 -0.179310  0.149701 -0.101817  \n",
       "PF15049  0.009382  0.090026  0.048761  0.020485 -0.080630  0.066559 -0.094392  \n",
       "PF11600  0.021495  0.198487 -0.043470  0.018098 -0.116805 -0.115569  0.051166  \n",
       "\n",
       "[12398 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model correctly using gensim\n",
    "pfam2vec_bin = Word2Vec.load('Word2Vec/word2vec_model.bin')\n",
    "\n",
    "# Convert to DataFrame\n",
    "pfam2vec = pd.DataFrame(pfam2vec_bin.wv.vectors, index=pfam2vec_bin.wv.index_to_key)\n",
    "\n",
    "# Show first two rows\n",
    "pfam2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>protein_id</th>\n",
       "      <th>gene_start</th>\n",
       "      <th>gene_end</th>\n",
       "      <th>gene_strand</th>\n",
       "      <th>pfam_id</th>\n",
       "      <th>in_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF02353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF01135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF01269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF13489</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BGC0000001.1</td>\n",
       "      <td>AEK75490.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "      <td>PF01596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96407</th>\n",
       "      <td>BGC0001833.1</td>\n",
       "      <td>AYA44686.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15051</td>\n",
       "      <td>1</td>\n",
       "      <td>PF13193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96408</th>\n",
       "      <td>BGC0001833.1</td>\n",
       "      <td>AYA44686.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15051</td>\n",
       "      <td>1</td>\n",
       "      <td>PF00668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96409</th>\n",
       "      <td>BGC0001833.1</td>\n",
       "      <td>AYA44686.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15051</td>\n",
       "      <td>1</td>\n",
       "      <td>PF00550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96410</th>\n",
       "      <td>BGC0001833.1</td>\n",
       "      <td>AYA44686.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15051</td>\n",
       "      <td>1</td>\n",
       "      <td>PF00975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96411</th>\n",
       "      <td>BGC0001833.1</td>\n",
       "      <td>AYA44686.1</td>\n",
       "      <td>0</td>\n",
       "      <td>15051</td>\n",
       "      <td>1</td>\n",
       "      <td>PF12697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96412 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sequence_id  protein_id  gene_start  gene_end  gene_strand  pfam_id  \\\n",
       "0      BGC0000001.1  AEK75490.1           0      1083            1  PF02353   \n",
       "1      BGC0000001.1  AEK75490.1           0      1083            1  PF01135   \n",
       "2      BGC0000001.1  AEK75490.1           0      1083            1  PF01269   \n",
       "3      BGC0000001.1  AEK75490.1           0      1083            1  PF13489   \n",
       "4      BGC0000001.1  AEK75490.1           0      1083            1  PF01596   \n",
       "...             ...         ...         ...       ...          ...      ...   \n",
       "96407  BGC0001833.1  AYA44686.1           0     15051            1  PF13193   \n",
       "96408  BGC0001833.1  AYA44686.1           0     15051            1  PF00668   \n",
       "96409  BGC0001833.1  AYA44686.1           0     15051            1  PF00550   \n",
       "96410  BGC0001833.1  AYA44686.1           0     15051            1  PF00975   \n",
       "96411  BGC0001833.1  AYA44686.1           0     15051            1  PF12697   \n",
       "\n",
       "       in_cluster  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "96407           1  \n",
       "96408           1  \n",
       "96409           1  \n",
       "96410           1  \n",
       "96411           1  \n",
       "\n",
       "[96412 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains = pd.read_csv('MIBiG.pfam.tsv', delimiter='\\t')\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1978"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contig_ids = domains['sequence_id'].unique()\n",
    "contig_ids = list(np.intersect1d(contig_ids, labels.index))\n",
    "len(contig_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BGC0001830.1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contig_ids[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_size_divisible(vectors):\n",
    "    missing_to_divisible = BATCH_SIZE - (len(vectors) % BATCH_SIZE)\n",
    "    shape = list(vectors.shape)\n",
    "    shape[0] = missing_to_divisible\n",
    "    print(missing_to_divisible)\n",
    "    return np.concatenate([vectors, np.zeros(shape=shape)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(1984, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alkaloid</th>\n",
       "      <th>NRP</th>\n",
       "      <th>Other</th>\n",
       "      <th>Polyketide</th>\n",
       "      <th>RiPP</th>\n",
       "      <th>Saccharide</th>\n",
       "      <th>Terpene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Alkaloid  NRP  Other  Polyketide  RiPP  Saccharide  Terpene\n",
       "0          0.0  0.0    0.0         1.0   0.0         0.0      0.0\n",
       "1          0.0  0.0    0.0         1.0   0.0         0.0      0.0\n",
       "2          0.0  0.0    0.0         1.0   0.0         0.0      0.0\n",
       "3          0.0  0.0    0.0         1.0   0.0         0.0      0.0\n",
       "4          0.0  0.0    0.0         1.0   0.0         0.0      0.0\n",
       "...        ...  ...    ...         ...   ...         ...      ...\n",
       "1979       0.0  0.0    0.0         0.0   0.0         0.0      0.0\n",
       "1980       0.0  0.0    0.0         0.0   0.0         0.0      0.0\n",
       "1981       0.0  0.0    0.0         0.0   0.0         0.0      0.0\n",
       "1982       0.0  0.0    0.0         0.0   0.0         0.0      0.0\n",
       "1983       0.0  0.0    0.0         0.0   0.0         0.0      0.0\n",
       "\n",
       "[1984 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_samples = labels.reindex(contig_ids).values\n",
    "y_samples = pd.DataFrame(make_batch_size_divisible(y_samples), columns=labels.columns)\n",
    "print(y_samples.shape)\n",
    "y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(1984, 128, 100)\n",
      "First sample shape: (128, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.25916222,  0.16687518,  0.45584118, ...,  0.23456158,\n",
       "         0.26959524, -0.38498661],\n",
       "       [ 0.00194787,  0.24177134,  0.01988547, ...,  0.225835  ,\n",
       "         0.05770348, -0.13457496],\n",
       "       [ 0.36128652,  0.64170277,  0.63306242, ...,  0.2202387 ,\n",
       "         0.71887439, -0.57087594],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_vectors = pfam2vec.reindex(domains['pfam_id'])\n",
    "domain_vectors['contig_id'] = domains['sequence_id'].values\n",
    "sample_vectors = domain_vectors.groupby('contig_id')\n",
    "X_samples = [sample_vectors.get_group(contig_id).drop('contig_id', axis=1).dropna() for contig_id in contig_ids]\n",
    "X_samples = pad_sequences(X_samples, maxlen=TIMESTEPS, dtype=np.float64, padding='post', truncating='post')\n",
    "X_samples = make_batch_size_divisible(X_samples)\n",
    "#X_samples = X_samples.reshape(-1, BATCH_SIZE, TIMESTEPS, PFAM2VEC_DIMENSIONS)\n",
    "print(X_samples.shape)\n",
    "print('First sample shape:', X_samples[0].shape)\n",
    "X_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317 48.59475806451613 142\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "maxi = 0\n",
    "avg = 0\n",
    "extra = 0\n",
    "for contig_id, contig_bgcs in sample_vectors:\n",
    "\n",
    "    maxi = max(maxi, len(contig_bgcs))\n",
    "    avg += len(contig_bgcs)\n",
    "    if(len(contig_bgcs) > 128): extra += 1\n",
    "    i += 1\n",
    "\n",
    "print(maxi, avg/i, extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BGC0000001       0\n",
       "BGC0000002       1\n",
       "BGC0000003       2\n",
       "BGC0000004       3\n",
       "BGC0000005       4\n",
       "              ... \n",
       "BGC0001826    1973\n",
       "BGC0001827    1974\n",
       "BGC0001828    1975\n",
       "BGC0001829    1976\n",
       "BGC0001830    1977\n",
       "Length: 1978, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_bgc_ids = pd.Series([contig_id.split('.')[0] for contig_id in contig_ids])\n",
    "major_to_minor_ids = pd.Series(range(0, len(contig_ids)), index=major_bgc_ids)\n",
    "major_to_minor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1806"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_major_ids = major_bgc_ids.unique()\n",
    "len(unique_major_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer\n",
    "\n",
    "def create_lstm(stacked_sizes=[], batched=True):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if batched:\n",
    "        # Add InputLayer with batch_input_shape\n",
    "        model.add(InputLayer(batch_input_shape=(BATCH_SIZE, TIMESTEPS, PFAM2VEC_DIMENSIONS)))\n",
    "    else:\n",
    "        # Add InputLayer with input_shape (no batch size)\n",
    "        model.add(InputLayer(input_shape=(TIMESTEPS, PFAM2VEC_DIMENSIONS)))\n",
    "\n",
    "    # Add Bidirectional LSTM layer\n",
    "    model.add(Bidirectional(\n",
    "        layer=LSTM(\n",
    "            units=16,\n",
    "            return_sequences=False,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            stateful=False\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    # Add stacked LSTM layers if specified\n",
    "    for size in stacked_sizes:\n",
    "        model.add(Bidirectional(LSTM(units=size, return_sequences=False, stateful=False)))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(NUM_LABELS, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv1D(stacked_sizes=[], batched=True):\n",
    "    model = Sequential()\n",
    "    args = {}\n",
    "    if batched:\n",
    "        args['batch_input_shape'] = (BATCH_SIZE, TIMESTEPS, 100)\n",
    "    else:\n",
    "        args['input_shape'] = (TIMESTEPS, 100)\n",
    "    model.add(Conv1D(128, 3, activation='relu', **args))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_1(create_model, epochs=200):\n",
    "    splitter = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "    scores = []\n",
    "    accuracies = []\n",
    "    best_accuracy = 0  # Track the best accuracy\n",
    "    best_model = None  # Track the best model\n",
    "\n",
    "    for fold, (id_train_idx, id_test_idx) in enumerate(splitter.split(unique_major_ids)):\n",
    "        train_major_ids, test_major_ids = unique_major_ids[id_train_idx], unique_major_ids[id_test_idx]\n",
    "        train_minor_ids, test_minor_ids = major_to_minor_ids.loc[train_major_ids], major_to_minor_ids.loc[test_major_ids]\n",
    "        X_train, X_test = X_samples[train_minor_ids], X_samples[test_minor_ids]\n",
    "        y_train, y_test = y_samples.loc[train_minor_ids].values, y_samples.loc[test_minor_ids].values\n",
    "        X_train = make_batch_size_divisible(X_train)\n",
    "        y_train = make_batch_size_divisible(y_train)\n",
    "        print('Train:', len(X_train), 'Test:', len(X_test))\n",
    "\n",
    "        # Create and train the model\n",
    "        train_model = create_model(batched=True)\n",
    "        train_model.fit(X_train, y_train, epochs=epochs, verbose=2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Create a model for evaluation\n",
    "        model = create_model(batched=False)\n",
    "        model.set_weights(train_model.get_weights())\n",
    "\n",
    "        pred = (model.predict(X_test) > 0.5).astype(np.int64)\n",
    "        aucs = metrics.roc_auc_score(y_test, pred, average=None)\n",
    "        precisions = metrics.precision_score(y_test, pred, average=None)\n",
    "        recalls = metrics.recall_score(y_test, pred, average=None)\n",
    "        accuracy = metrics.accuracy_score(y_test, pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Append scores for each class\n",
    "        scores += [{'Precision': p, 'Recall': r, 'AUC': a, 'Class': c, 'Samples': int(sum(y_samples[c]))} \n",
    "                   for p, r, a, c in zip(precisions, recalls, aucs, y_samples.columns)]\n",
    "\n",
    "        # Track the best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = train_model  # Save the best model\n",
    "\n",
    "    # Save the best model after all folds are complete\n",
    "    if best_model is not None:\n",
    "        best_model.save(\"2.best_model.h5\")\n",
    "        print(f\"Best model saved with accuracy: {best_accuracy}\")\n",
    "\n",
    "    scores = pd.DataFrame(scores).groupby('Class').mean().sort_values('Samples', ascending=False)\n",
    "    print('Accuracy (exact match)', np.mean(accuracies))\n",
    "    print(scores.mean())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "Train: 1584 Test: 407\n",
      "Epoch 1/100\n",
      "99/99 - 6s - 58ms/step - loss: 2.1412\n",
      "Epoch 2/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.9205\n",
      "Epoch 3/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.7134\n",
      "Epoch 4/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.5816\n",
      "Epoch 5/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.4770\n",
      "Epoch 6/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.4113\n",
      "Epoch 7/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.3426\n",
      "Epoch 8/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.3109\n",
      "Epoch 9/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.2837\n",
      "Epoch 10/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.2485\n",
      "Epoch 11/100\n",
      "99/99 - 4s - 44ms/step - loss: 1.2379\n",
      "Epoch 12/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.2600\n",
      "Epoch 13/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2109\n",
      "Epoch 14/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2388\n",
      "Epoch 15/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2248\n",
      "Epoch 16/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2026\n",
      "Epoch 17/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2237\n",
      "Epoch 18/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2021\n",
      "Epoch 19/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2255\n",
      "Epoch 20/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2142\n",
      "Epoch 21/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2076\n",
      "Epoch 22/100\n",
      "99/99 - 5s - 51ms/step - loss: 1.2246\n",
      "Epoch 23/100\n",
      "99/99 - 5s - 46ms/step - loss: 1.2234\n",
      "Epoch 24/100\n",
      "99/99 - 5s - 50ms/step - loss: 1.2165\n",
      "Epoch 25/100\n",
      "99/99 - 4s - 45ms/step - loss: 1.2211\n",
      "Epoch 26/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.2395\n",
      "Epoch 27/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2326\n",
      "Epoch 28/100\n",
      "99/99 - 5s - 47ms/step - loss: 1.2361\n",
      "Epoch 29/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.2220\n",
      "Epoch 30/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2520\n",
      "Epoch 31/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.2459\n",
      "Epoch 32/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2528\n",
      "Epoch 33/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2551\n",
      "Epoch 34/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.2493\n",
      "Epoch 35/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.2638\n",
      "Epoch 36/100\n",
      "99/99 - 4s - 45ms/step - loss: 1.2728\n",
      "Epoch 37/100\n",
      "99/99 - 4s - 45ms/step - loss: 1.3088\n",
      "Epoch 38/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.3023\n",
      "Epoch 39/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.3121\n",
      "Epoch 40/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.3207\n",
      "Epoch 41/100\n",
      "99/99 - 5s - 50ms/step - loss: 1.3145\n",
      "Epoch 42/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.3423\n",
      "Epoch 43/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3530\n",
      "Epoch 44/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.3799\n",
      "Epoch 45/100\n",
      "99/99 - 4s - 44ms/step - loss: 1.3828\n",
      "Epoch 46/100\n",
      "99/99 - 5s - 49ms/step - loss: 1.3633\n",
      "Epoch 47/100\n",
      "99/99 - 5s - 47ms/step - loss: 1.3618\n",
      "Epoch 48/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.3462\n",
      "Epoch 49/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.4197\n",
      "Epoch 50/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.4194\n",
      "Epoch 51/100\n",
      "99/99 - 5s - 47ms/step - loss: 1.4079\n",
      "Epoch 52/100\n",
      "99/99 - 4s - 45ms/step - loss: 1.3970\n",
      "Epoch 53/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.4424\n",
      "Epoch 54/100\n",
      "99/99 - 5s - 48ms/step - loss: 1.4282\n",
      "Epoch 55/100\n",
      "99/99 - 4s - 44ms/step - loss: 1.4419\n",
      "Epoch 56/100\n",
      "99/99 - 4s - 45ms/step - loss: 1.5051\n",
      "Epoch 57/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.4794\n",
      "Epoch 58/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.4701\n",
      "Epoch 59/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.5061\n",
      "Epoch 60/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4888\n",
      "Epoch 61/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5315\n",
      "Epoch 62/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.5332\n",
      "Epoch 63/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5060\n",
      "Epoch 64/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.5646\n",
      "Epoch 65/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.4955\n",
      "Epoch 66/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5296\n",
      "Epoch 67/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.5213\n",
      "Epoch 68/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.5857\n",
      "Epoch 69/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5788\n",
      "Epoch 70/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5973\n",
      "Epoch 71/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5961\n",
      "Epoch 72/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6482\n",
      "Epoch 73/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.6089\n",
      "Epoch 74/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.6651\n",
      "Epoch 75/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6001\n",
      "Epoch 76/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.6255\n",
      "Epoch 77/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6290\n",
      "Epoch 78/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6717\n",
      "Epoch 79/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6492\n",
      "Epoch 80/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.6711\n",
      "Epoch 81/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.7071\n",
      "Epoch 82/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.7316\n",
      "Epoch 83/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.7115\n",
      "Epoch 84/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.6854\n",
      "Epoch 85/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.7416\n",
      "Epoch 86/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.7530\n",
      "Epoch 87/100\n",
      "99/99 - 5s - 48ms/step - loss: 1.6740\n",
      "Epoch 88/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.7351\n",
      "Epoch 89/100\n",
      "99/99 - 4s - 41ms/step - loss: 1.7533\n",
      "Epoch 90/100\n",
      "99/99 - 4s - 44ms/step - loss: 1.7261\n",
      "Epoch 91/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.6960\n",
      "Epoch 92/100\n",
      "99/99 - 4s - 44ms/step - loss: 1.7216\n",
      "Epoch 93/100\n",
      "99/99 - 4s - 44ms/step - loss: 1.7830\n",
      "Epoch 94/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.8487\n",
      "Epoch 95/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.7872\n",
      "Epoch 96/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.8281\n",
      "Epoch 97/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.8393\n",
      "Epoch 98/100\n",
      "99/99 - 4s - 42ms/step - loss: 1.8296\n",
      "Epoch 99/100\n",
      "99/99 - 5s - 49ms/step - loss: 1.8012\n",
      "Epoch 100/100\n",
      "99/99 - 5s - 51ms/step - loss: 1.8355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ultron/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n",
      "14\n",
      "14\n",
      "Train: 1600 Test: 392\n",
      "Epoch 1/100\n",
      "100/100 - 7s - 65ms/step - loss: 2.0727\n",
      "Epoch 2/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.8916\n",
      "Epoch 3/100\n",
      "100/100 - 5s - 50ms/step - loss: 1.6779\n",
      "Epoch 4/100\n",
      "100/100 - 5s - 50ms/step - loss: 1.5217\n",
      "Epoch 5/100\n",
      "100/100 - 5s - 49ms/step - loss: 1.4244\n",
      "Epoch 6/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.3840\n",
      "Epoch 7/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.3210\n",
      "Epoch 8/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.2936\n",
      "Epoch 9/100\n",
      "100/100 - 4s - 45ms/step - loss: 1.2873\n",
      "Epoch 10/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2812\n",
      "Epoch 11/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2600\n",
      "Epoch 12/100\n",
      "100/100 - 5s - 52ms/step - loss: 1.2793\n",
      "Epoch 13/100\n",
      "100/100 - 5s - 50ms/step - loss: 1.2503\n",
      "Epoch 14/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.2498\n",
      "Epoch 15/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.2662\n",
      "Epoch 16/100\n",
      "100/100 - 6s - 58ms/step - loss: 1.2402\n",
      "Epoch 17/100\n",
      "100/100 - 5s - 53ms/step - loss: 1.2490\n",
      "Epoch 18/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2291\n",
      "Epoch 19/100\n",
      "100/100 - 4s - 45ms/step - loss: 1.2783\n",
      "Epoch 20/100\n",
      "100/100 - 5s - 49ms/step - loss: 1.2228\n",
      "Epoch 21/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.2566\n",
      "Epoch 22/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.2787\n",
      "Epoch 23/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.2221\n",
      "Epoch 24/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2377\n",
      "Epoch 25/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.2820\n",
      "Epoch 26/100\n",
      "100/100 - 4s - 45ms/step - loss: 1.2572\n",
      "Epoch 27/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.2403\n",
      "Epoch 28/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.2557\n",
      "Epoch 29/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2652\n",
      "Epoch 30/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2689\n",
      "Epoch 31/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.2886\n",
      "Epoch 32/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2682\n",
      "Epoch 33/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2677\n",
      "Epoch 34/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.2853\n",
      "Epoch 35/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.2942\n",
      "Epoch 36/100\n",
      "100/100 - 4s - 44ms/step - loss: 1.2653\n",
      "Epoch 37/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.3486\n",
      "Epoch 38/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.2991\n",
      "Epoch 39/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.2766\n",
      "Epoch 40/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.2936\n",
      "Epoch 41/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.3634\n",
      "Epoch 42/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.3125\n",
      "Epoch 43/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.3030\n",
      "Epoch 44/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.3209\n",
      "Epoch 45/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.3228\n",
      "Epoch 46/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.3373\n",
      "Epoch 47/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.3305\n",
      "Epoch 48/100\n",
      "100/100 - 5s - 51ms/step - loss: 1.3081\n",
      "Epoch 49/100\n",
      "100/100 - 5s - 49ms/step - loss: 1.3367\n",
      "Epoch 50/100\n",
      "100/100 - 4s - 38ms/step - loss: 1.3385\n",
      "Epoch 51/100\n",
      "100/100 - 4s - 36ms/step - loss: 1.3789\n",
      "Epoch 52/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.3479\n",
      "Epoch 53/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.3489\n",
      "Epoch 54/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.3735\n",
      "Epoch 55/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.3831\n",
      "Epoch 56/100\n",
      "100/100 - 5s - 53ms/step - loss: 1.3747\n",
      "Epoch 57/100\n",
      "100/100 - 6s - 56ms/step - loss: 1.3579\n",
      "Epoch 58/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.3699\n",
      "Epoch 59/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.3521\n",
      "Epoch 60/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.3974\n",
      "Epoch 61/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.4112\n",
      "Epoch 62/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.3618\n",
      "Epoch 63/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.4305\n",
      "Epoch 64/100\n",
      "100/100 - 5s - 52ms/step - loss: 1.4087\n",
      "Epoch 65/100\n",
      "100/100 - 5s - 49ms/step - loss: 1.4286\n",
      "Epoch 66/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.4081\n",
      "Epoch 67/100\n",
      "100/100 - 5s - 50ms/step - loss: 1.4734\n",
      "Epoch 68/100\n",
      "100/100 - 5s - 51ms/step - loss: 1.4435\n",
      "Epoch 69/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.4413\n",
      "Epoch 70/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.4747\n",
      "Epoch 71/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.4680\n",
      "Epoch 72/100\n",
      "100/100 - 5s - 51ms/step - loss: 1.4740\n",
      "Epoch 73/100\n",
      "100/100 - 5s - 54ms/step - loss: 1.4699\n",
      "Epoch 74/100\n",
      "100/100 - 5s - 50ms/step - loss: 1.4568\n",
      "Epoch 75/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.4281\n",
      "Epoch 76/100\n",
      "100/100 - 5s - 52ms/step - loss: 1.4584\n",
      "Epoch 77/100\n",
      "100/100 - 5s - 55ms/step - loss: 1.4701\n",
      "Epoch 78/100\n",
      "100/100 - 5s - 51ms/step - loss: 1.4692\n",
      "Epoch 79/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.5159\n",
      "Epoch 80/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.4720\n",
      "Epoch 81/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.5057\n",
      "Epoch 82/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.5210\n",
      "Epoch 83/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.4868\n",
      "Epoch 84/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.5481\n",
      "Epoch 85/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.5550\n",
      "Epoch 86/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.5310\n",
      "Epoch 87/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.5061\n",
      "Epoch 88/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.5531\n",
      "Epoch 89/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.5926\n",
      "Epoch 90/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.5770\n",
      "Epoch 91/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.5856\n",
      "Epoch 92/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.5589\n",
      "Epoch 93/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.5832\n",
      "Epoch 94/100\n",
      "100/100 - 5s - 45ms/step - loss: 1.5583\n",
      "Epoch 95/100\n",
      "100/100 - 5s - 47ms/step - loss: 1.6011\n",
      "Epoch 96/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.5512\n",
      "Epoch 97/100\n",
      "100/100 - 5s - 48ms/step - loss: 1.6075\n",
      "Epoch 98/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.6462\n",
      "Epoch 99/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.5995\n",
      "Epoch 100/100\n",
      "100/100 - 5s - 46ms/step - loss: 1.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ultron/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
      "7\n",
      "7\n",
      "Train: 1616 Test: 369\n",
      "Epoch 1/100\n",
      "101/101 - 9s - 84ms/step - loss: 2.0473\n",
      "Epoch 2/100\n",
      "101/101 - 6s - 54ms/step - loss: 1.8459\n",
      "Epoch 3/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.6458\n",
      "Epoch 4/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.5493\n",
      "Epoch 5/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.5115\n",
      "Epoch 6/100\n",
      "101/101 - 5s - 48ms/step - loss: 1.4293\n",
      "Epoch 7/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.3739\n",
      "Epoch 8/100\n",
      "101/101 - 5s - 48ms/step - loss: 1.3341\n",
      "Epoch 9/100\n",
      "101/101 - 5s - 49ms/step - loss: 1.2983\n",
      "Epoch 10/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.2764\n",
      "Epoch 11/100\n",
      "101/101 - 5s - 49ms/step - loss: 1.2383\n",
      "Epoch 12/100\n",
      "101/101 - 5s - 54ms/step - loss: 1.2291\n",
      "Epoch 13/100\n",
      "101/101 - 7s - 68ms/step - loss: 1.2238\n",
      "Epoch 14/100\n",
      "101/101 - 6s - 57ms/step - loss: 1.2073\n",
      "Epoch 15/100\n",
      "101/101 - 5s - 54ms/step - loss: 1.2116\n",
      "Epoch 16/100\n",
      "101/101 - 5s - 49ms/step - loss: 1.2152\n",
      "Epoch 17/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.1828\n",
      "Epoch 18/100\n",
      "101/101 - 5s - 45ms/step - loss: 1.1900\n",
      "Epoch 19/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.1766\n",
      "Epoch 20/100\n",
      "101/101 - 5s - 49ms/step - loss: 1.1840\n",
      "Epoch 21/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.2160\n",
      "Epoch 22/100\n",
      "101/101 - 5s - 46ms/step - loss: 1.1947\n",
      "Epoch 23/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.2261\n",
      "Epoch 24/100\n",
      "101/101 - 5s - 47ms/step - loss: 1.2086\n",
      "Epoch 25/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.2258\n",
      "Epoch 26/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.2313\n",
      "Epoch 27/100\n",
      "101/101 - 5s - 49ms/step - loss: 1.2348\n",
      "Epoch 28/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.2430\n",
      "Epoch 29/100\n",
      "101/101 - 4s - 41ms/step - loss: 1.2092\n",
      "Epoch 30/100\n",
      "101/101 - 4s - 43ms/step - loss: 1.2114\n",
      "Epoch 31/100\n",
      "101/101 - 5s - 52ms/step - loss: 1.2359\n",
      "Epoch 32/100\n",
      "101/101 - 6s - 55ms/step - loss: 1.2574\n",
      "Epoch 33/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.2818\n",
      "Epoch 34/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.2704\n",
      "Epoch 35/100\n",
      "101/101 - 5s - 52ms/step - loss: 1.2623\n",
      "Epoch 36/100\n",
      "101/101 - 5s - 53ms/step - loss: 1.2908\n",
      "Epoch 37/100\n",
      "101/101 - 5s - 53ms/step - loss: 1.3089\n",
      "Epoch 38/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.2937\n",
      "Epoch 39/100\n",
      "101/101 - 5s - 52ms/step - loss: 1.3049\n",
      "Epoch 40/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.2804\n",
      "Epoch 41/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.2803\n",
      "Epoch 42/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.2879\n",
      "Epoch 43/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.3222\n",
      "Epoch 44/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.3236\n",
      "Epoch 45/100\n",
      "101/101 - 5s - 52ms/step - loss: 1.3383\n",
      "Epoch 46/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.3074\n",
      "Epoch 47/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.3238\n",
      "Epoch 48/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.3305\n",
      "Epoch 49/100\n",
      "101/101 - 5s - 54ms/step - loss: 1.3173\n",
      "Epoch 50/100\n",
      "101/101 - 5s - 50ms/step - loss: 1.3367\n",
      "Epoch 51/100\n",
      "101/101 - 6s - 55ms/step - loss: 1.3515\n",
      "Epoch 52/100\n",
      "101/101 - 5s - 52ms/step - loss: 1.3360\n",
      "Epoch 53/100\n",
      "101/101 - 5s - 51ms/step - loss: 1.3712\n",
      "Epoch 54/100\n",
      "101/101 - 5s - 53ms/step - loss: 1.3876\n",
      "Epoch 55/100\n",
      "101/101 - 5s - 53ms/step - loss: 1.4182\n",
      "Epoch 56/100\n",
      "101/101 - 6s - 55ms/step - loss: 1.3690\n",
      "Epoch 57/100\n",
      "101/101 - 4s - 41ms/step - loss: 1.4070\n",
      "Epoch 58/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.4242\n",
      "Epoch 59/100\n",
      "101/101 - 4s - 42ms/step - loss: 1.3927\n",
      "Epoch 60/100\n",
      "101/101 - 4s - 41ms/step - loss: 1.3777\n",
      "Epoch 61/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.3882\n",
      "Epoch 62/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.4205\n",
      "Epoch 63/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.4505\n",
      "Epoch 64/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.4160\n",
      "Epoch 65/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.4269\n",
      "Epoch 66/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.4682\n",
      "Epoch 67/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.4070\n",
      "Epoch 68/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.4663\n",
      "Epoch 69/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.5046\n",
      "Epoch 70/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.4696\n",
      "Epoch 71/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.4894\n",
      "Epoch 72/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.4957\n",
      "Epoch 73/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.4832\n",
      "Epoch 74/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.4857\n",
      "Epoch 75/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.5407\n",
      "Epoch 76/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.5446\n",
      "Epoch 77/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.6185\n",
      "Epoch 78/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.5617\n",
      "Epoch 79/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.5286\n",
      "Epoch 80/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.5361\n",
      "Epoch 81/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.5573\n",
      "Epoch 82/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.5598\n",
      "Epoch 83/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.5948\n",
      "Epoch 84/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.5499\n",
      "Epoch 85/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.5891\n",
      "Epoch 86/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.5920\n",
      "Epoch 87/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.5928\n",
      "Epoch 88/100\n",
      "101/101 - 4s - 40ms/step - loss: 1.5773\n",
      "Epoch 89/100\n",
      "101/101 - 4s - 37ms/step - loss: 1.5638\n",
      "Epoch 90/100\n",
      "101/101 - 4s - 37ms/step - loss: 1.6860\n",
      "Epoch 91/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.5836\n",
      "Epoch 92/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.6464\n",
      "Epoch 93/100\n",
      "101/101 - 4s - 38ms/step - loss: 1.6285\n",
      "Epoch 94/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.6652\n",
      "Epoch 95/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.6379\n",
      "Epoch 96/100\n",
      "101/101 - 4s - 37ms/step - loss: 1.6366\n",
      "Epoch 97/100\n",
      "101/101 - 4s - 37ms/step - loss: 1.6459\n",
      "Epoch 98/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.7024\n",
      "Epoch 99/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.6634\n",
      "Epoch 100/100\n",
      "101/101 - 4s - 39ms/step - loss: 1.6769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ultron/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "4\n",
      "4\n",
      "Train: 1584 Test: 398\n",
      "Epoch 1/100\n",
      "99/99 - 5s - 55ms/step - loss: 2.1830\n",
      "Epoch 2/100\n",
      "99/99 - 4s - 38ms/step - loss: 2.0126\n",
      "Epoch 3/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.7950\n",
      "Epoch 4/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6200\n",
      "Epoch 5/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5114\n",
      "Epoch 6/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4466\n",
      "Epoch 7/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.4301\n",
      "Epoch 8/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3498\n",
      "Epoch 9/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.3214\n",
      "Epoch 10/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.3111\n",
      "Epoch 11/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.2972\n",
      "Epoch 12/100\n",
      "99/99 - 4s - 35ms/step - loss: 1.3324\n",
      "Epoch 13/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3236\n",
      "Epoch 14/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.2946\n",
      "Epoch 15/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3133\n",
      "Epoch 16/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3207\n",
      "Epoch 17/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3264\n",
      "Epoch 18/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3178\n",
      "Epoch 19/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3068\n",
      "Epoch 20/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3199\n",
      "Epoch 21/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3285\n",
      "Epoch 22/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.3349\n",
      "Epoch 23/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3109\n",
      "Epoch 24/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3501\n",
      "Epoch 25/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3513\n",
      "Epoch 26/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3076\n",
      "Epoch 27/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3505\n",
      "Epoch 28/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3646\n",
      "Epoch 29/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3111\n",
      "Epoch 30/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3716\n",
      "Epoch 31/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3708\n",
      "Epoch 32/100\n",
      "99/99 - 4s - 40ms/step - loss: 1.3614\n",
      "Epoch 33/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3910\n",
      "Epoch 34/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.3571\n",
      "Epoch 35/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3524\n",
      "Epoch 36/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.3609\n",
      "Epoch 37/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3775\n",
      "Epoch 38/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.4311\n",
      "Epoch 39/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.3656\n",
      "Epoch 40/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.4099\n",
      "Epoch 41/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.4260\n",
      "Epoch 42/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4163\n",
      "Epoch 43/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.3961\n",
      "Epoch 44/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.4535\n",
      "Epoch 45/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.5147\n",
      "Epoch 46/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5027\n",
      "Epoch 47/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4619\n",
      "Epoch 48/100\n",
      "99/99 - 4s - 43ms/step - loss: 1.4969\n",
      "Epoch 49/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4521\n",
      "Epoch 50/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4757\n",
      "Epoch 51/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.4711\n",
      "Epoch 52/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.4653\n",
      "Epoch 53/100\n",
      "99/99 - 3s - 34ms/step - loss: 1.4382\n",
      "Epoch 54/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.4813\n",
      "Epoch 55/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5450\n",
      "Epoch 56/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.5009\n",
      "Epoch 57/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5634\n",
      "Epoch 58/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5077\n",
      "Epoch 59/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.4813\n",
      "Epoch 60/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.5828\n",
      "Epoch 61/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6012\n",
      "Epoch 62/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5342\n",
      "Epoch 63/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5567\n",
      "Epoch 64/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5593\n",
      "Epoch 65/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.5839\n",
      "Epoch 66/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.5686\n",
      "Epoch 67/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5550\n",
      "Epoch 68/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.5958\n",
      "Epoch 69/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.5874\n",
      "Epoch 70/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6103\n",
      "Epoch 71/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6130\n",
      "Epoch 72/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6359\n",
      "Epoch 73/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6132\n",
      "Epoch 74/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6476\n",
      "Epoch 75/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6448\n",
      "Epoch 76/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6415\n",
      "Epoch 77/100\n",
      "99/99 - 3s - 35ms/step - loss: 1.6772\n",
      "Epoch 78/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6357\n",
      "Epoch 79/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6129\n",
      "Epoch 80/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6440\n",
      "Epoch 81/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.6795\n",
      "Epoch 82/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6153\n",
      "Epoch 83/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.7146\n",
      "Epoch 84/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.5756\n",
      "Epoch 85/100\n",
      "99/99 - 3s - 35ms/step - loss: 1.6327\n",
      "Epoch 86/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.6661\n",
      "Epoch 87/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6196\n",
      "Epoch 88/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.7183\n",
      "Epoch 89/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6812\n",
      "Epoch 90/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.7548\n",
      "Epoch 91/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.7058\n",
      "Epoch 92/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.7032\n",
      "Epoch 93/100\n",
      "99/99 - 4s - 38ms/step - loss: 1.7128\n",
      "Epoch 94/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.7675\n",
      "Epoch 95/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.7574\n",
      "Epoch 96/100\n",
      "99/99 - 4s - 39ms/step - loss: 1.7256\n",
      "Epoch 97/100\n",
      "99/99 - 4s - 36ms/step - loss: 1.7210\n",
      "Epoch 98/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.6996\n",
      "Epoch 99/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.7652\n",
      "Epoch 100/100\n",
      "99/99 - 4s - 37ms/step - loss: 1.7921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ultron/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step\n",
      "2\n",
      "2\n",
      "Train: 1568 Test: 412\n",
      "Epoch 1/100\n",
      "98/98 - 5s - 55ms/step - loss: 2.0356\n",
      "Epoch 2/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.8687\n",
      "Epoch 3/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.6726\n",
      "Epoch 4/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.5686\n",
      "Epoch 5/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.5136\n",
      "Epoch 6/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.5049\n",
      "Epoch 7/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.4300\n",
      "Epoch 8/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.4437\n",
      "Epoch 9/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.4036\n",
      "Epoch 10/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.4032\n",
      "Epoch 11/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.3766\n",
      "Epoch 12/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.3333\n",
      "Epoch 13/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.3397\n",
      "Epoch 14/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.3034\n",
      "Epoch 15/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.3021\n",
      "Epoch 16/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.2724\n",
      "Epoch 17/100\n",
      "98/98 - 4s - 41ms/step - loss: 1.2515\n",
      "Epoch 18/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.2472\n",
      "Epoch 19/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.2293\n",
      "Epoch 20/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.2432\n",
      "Epoch 21/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.2198\n",
      "Epoch 22/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.2198\n",
      "Epoch 23/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.2096\n",
      "Epoch 24/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.2390\n",
      "Epoch 25/100\n",
      "98/98 - 5s - 50ms/step - loss: 1.2418\n",
      "Epoch 26/100\n",
      "98/98 - 5s - 53ms/step - loss: 1.2267\n",
      "Epoch 27/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.2164\n",
      "Epoch 28/100\n",
      "98/98 - 5s - 51ms/step - loss: 1.2370\n",
      "Epoch 29/100\n",
      "98/98 - 5s - 53ms/step - loss: 1.2376\n",
      "Epoch 30/100\n",
      "98/98 - 6s - 63ms/step - loss: 1.2300\n",
      "Epoch 31/100\n",
      "98/98 - 4s - 45ms/step - loss: 1.2296\n",
      "Epoch 32/100\n",
      "98/98 - 4s - 44ms/step - loss: 1.2587\n",
      "Epoch 33/100\n",
      "98/98 - 5s - 54ms/step - loss: 1.2503\n",
      "Epoch 34/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.2425\n",
      "Epoch 35/100\n",
      "98/98 - 5s - 51ms/step - loss: 1.2585\n",
      "Epoch 36/100\n",
      "98/98 - 5s - 51ms/step - loss: 1.2552\n",
      "Epoch 37/100\n",
      "98/98 - 5s - 51ms/step - loss: 1.2231\n",
      "Epoch 38/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.2819\n",
      "Epoch 39/100\n",
      "98/98 - 3s - 35ms/step - loss: 1.2729\n",
      "Epoch 40/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.2911\n",
      "Epoch 41/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.3059\n",
      "Epoch 42/100\n",
      "98/98 - 5s - 50ms/step - loss: 1.2894\n",
      "Epoch 43/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.3083\n",
      "Epoch 44/100\n",
      "98/98 - 5s - 48ms/step - loss: 1.2950\n",
      "Epoch 45/100\n",
      "98/98 - 4s - 44ms/step - loss: 1.2919\n",
      "Epoch 46/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.3236\n",
      "Epoch 47/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.3380\n",
      "Epoch 48/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.3235\n",
      "Epoch 49/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.3094\n",
      "Epoch 50/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.3345\n",
      "Epoch 51/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.3445\n",
      "Epoch 52/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.3460\n",
      "Epoch 53/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.3959\n",
      "Epoch 54/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.3834\n",
      "Epoch 55/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.3887\n",
      "Epoch 56/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.4065\n",
      "Epoch 57/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.3722\n",
      "Epoch 58/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.3931\n",
      "Epoch 59/100\n",
      "98/98 - 5s - 51ms/step - loss: 1.4502\n",
      "Epoch 60/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.3788\n",
      "Epoch 61/100\n",
      "98/98 - 5s - 50ms/step - loss: 1.4238\n",
      "Epoch 62/100\n",
      "98/98 - 5s - 49ms/step - loss: 1.4106\n",
      "Epoch 63/100\n",
      "98/98 - 5s - 50ms/step - loss: 1.4340\n",
      "Epoch 64/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.4417\n",
      "Epoch 65/100\n",
      "98/98 - 4s - 36ms/step - loss: 1.4304\n",
      "Epoch 66/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.4713\n",
      "Epoch 67/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.4615\n",
      "Epoch 68/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.4964\n",
      "Epoch 69/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.4462\n",
      "Epoch 70/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.4752\n",
      "Epoch 71/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.4922\n",
      "Epoch 72/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.4966\n",
      "Epoch 73/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.4780\n",
      "Epoch 74/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.4619\n",
      "Epoch 75/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.5013\n",
      "Epoch 76/100\n",
      "98/98 - 5s - 50ms/step - loss: 1.5250\n",
      "Epoch 77/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.5534\n",
      "Epoch 78/100\n",
      "98/98 - 5s - 51ms/step - loss: 1.5245\n",
      "Epoch 79/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.5705\n",
      "Epoch 80/100\n",
      "98/98 - 5s - 53ms/step - loss: 1.5297\n",
      "Epoch 81/100\n",
      "98/98 - 5s - 52ms/step - loss: 1.5062\n",
      "Epoch 82/100\n",
      "98/98 - 4s - 40ms/step - loss: 1.5916\n",
      "Epoch 83/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.5429\n",
      "Epoch 84/100\n",
      "98/98 - 3s - 36ms/step - loss: 1.6288\n",
      "Epoch 85/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.6069\n",
      "Epoch 86/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.6272\n",
      "Epoch 87/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.5816\n",
      "Epoch 88/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.5992\n",
      "Epoch 89/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.6194\n",
      "Epoch 90/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.6193\n",
      "Epoch 91/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.6159\n",
      "Epoch 92/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.7098\n",
      "Epoch 93/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.6855\n",
      "Epoch 94/100\n",
      "98/98 - 4s - 38ms/step - loss: 1.6687\n",
      "Epoch 95/100\n",
      "98/98 - 4s - 37ms/step - loss: 1.6694\n",
      "Epoch 96/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.7091\n",
      "Epoch 97/100\n",
      "98/98 - 4s - 42ms/step - loss: 1.6873\n",
      "Epoch 98/100\n",
      "98/98 - 4s - 39ms/step - loss: 1.6974\n",
      "Epoch 99/100\n",
      "98/98 - 4s - 41ms/step - loss: 1.6344\n",
      "Epoch 100/100\n",
      "98/98 - 4s - 41ms/step - loss: 1.7396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ultron/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with accuracy: 0.6422764227642277\n",
      "Accuracy (exact match) 0.628609405005581\n",
      "Precision      0.746076\n",
      "Recall         0.570213\n",
      "AUC            0.770637\n",
      "Samples      331.857143\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_1(create_lstm, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Polyketide</th>\n",
       "      <td>0.879722</td>\n",
       "      <td>0.871058</td>\n",
       "      <td>0.892950</td>\n",
       "      <td>824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRP</th>\n",
       "      <td>0.869225</td>\n",
       "      <td>0.500820</td>\n",
       "      <td>0.734256</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RiPP</th>\n",
       "      <td>0.913922</td>\n",
       "      <td>0.837501</td>\n",
       "      <td>0.913338</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.646218</td>\n",
       "      <td>0.491168</td>\n",
       "      <td>0.726341</td>\n",
       "      <td>246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saccharide</th>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.627468</td>\n",
       "      <td>0.808512</td>\n",
       "      <td>187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terpene</th>\n",
       "      <td>0.699433</td>\n",
       "      <td>0.583790</td>\n",
       "      <td>0.781534</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alkaloid</th>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.079683</td>\n",
       "      <td>0.537527</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall       AUC  Samples\n",
       "Class                                             \n",
       "Polyketide   0.879722  0.871058  0.892950    824.0\n",
       "NRP          0.869225  0.500820  0.734256    600.0\n",
       "RiPP         0.913922  0.837501  0.913338    254.0\n",
       "Other        0.646218  0.491168  0.726341    246.0\n",
       "Saccharide   0.874016  0.627468  0.808512    187.0\n",
       "Terpene      0.699433  0.583790  0.781534    158.0\n",
       "Alkaloid     0.340000  0.079683  0.537527     54.0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Precision      0.746076\n",
       "Recall         0.570213\n",
       "AUC            0.770637\n",
       "Samples      331.857143\n",
       "dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
